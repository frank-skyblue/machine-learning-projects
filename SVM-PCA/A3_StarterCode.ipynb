{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J1grKKjV-nzO",
    "outputId": "2b07549d-7ba6-4e10-dd9d-b232e58d6d35"
   },
   "outputs": [],
   "source": [
    "# pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0CQf65BS-nzT",
    "outputId": "37de0002-4906-4e99-a47f-609be2f2fade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease install this specific version of resampy for librosa to work without errors.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Please install this specific version of resampy for librosa to work without errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7FE1qM7n-nzU",
    "outputId": "02c7b706-8c9c-42b4-ed25-b98f7cd915de"
   },
   "outputs": [],
   "source": [
    "# pip install resampy==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_hwFIUlFzTYw",
    "outputId": "9419a67b-5c1d-47e0-a7f5-ec9b9cad5bca"
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GWrZdqrp2E98"
   },
   "outputs": [],
   "source": [
    "emotions ={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrgQMSEU-nzZ"
   },
   "source": [
    "### Data for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vSe0sBT12HgO"
   },
   "outputs": [],
   "source": [
    "def load_extract_features(data_path):\n",
    "\n",
    "    '''\n",
    "    load_extract_features() is a function that is used to load all the audio files one at a time, compute their features and return the features as well as the target values.\n",
    "\n",
    "    There are around 8-10 audio files which are corrupted. We hardcode zero values for such files in order to maintain consistency.\n",
    "\n",
    "    ['calm', 'happy'] emotion data is categorized into 'positive' and  ['angry', 'fearful'] into 'negative'\n",
    "\n",
    "    Returns:\n",
    "    1. Features\n",
    "    2. Binary Target Values\n",
    "    '''\n",
    "    final_features,target_emotions, binary_label = [],[],[]\n",
    "    count = 0\n",
    "    \n",
    "    for i in glob.glob(data_path + \"/Actor_*/*.wav\"): #Loop to read every file.\n",
    "        \n",
    "        name = os.path.basename(i)\n",
    "        #We split the name of the file to understand the emotion associated with the file.\n",
    "        split = name.split(\"-\")\n",
    "        #We know that the third identifier is associated with the emotion of the audio file. Hence, we use [2] as it represents the third identifier.\n",
    "        emotion = emotions[split[2]]\n",
    "\n",
    "        #Below is the code to categorize the emotions into two classes to make this a binary problem.\n",
    "        if emotion in ['calm', 'happy']:\n",
    "            binary_label.append(-1)\n",
    "        elif emotion in ['angry', 'fearful']:\n",
    "            binary_label.append(1)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        with soundfile.SoundFile(i) as audio:\n",
    "            waveform = audio.read(dtype=\"float32\")\n",
    "            sr = audio.samplerate\n",
    "            \n",
    "            #Below is the code to extract the Mel spectrogram features\n",
    "            #128 is the standard for machine learning applications using Mel spectrograms\n",
    "            m_feature = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128, fmax=sr / 2.0).T\n",
    "            melspectrogram = np.mean(m_feature,axis=0)\n",
    "            if melspectrogram.shape != (128,):\n",
    "                melspectrogram = np.zeros(128)\n",
    "            \n",
    "            #Below is the code to extract the chromagram features\n",
    "            stft_wave = librosa.stft(waveform)\n",
    "            stft = np.abs(stft_wave)\n",
    "            c_feature = librosa.feature.chroma_stft(S=stft, sr=sr).T\n",
    "            chromagram = np.mean(c_feature,axis=0)\n",
    "            \n",
    "            #12 is the number of pitch classes\n",
    "            if chromagram.shape != (12,):\n",
    "                chromagram = np.zeros(12)\n",
    "                \n",
    "            features=np.array([])\n",
    "            features=np.hstack((chromagram, melspectrogram))\n",
    "        \n",
    "            final_features.append(features)\n",
    "            target_emotions.append(emotion)\n",
    "            \n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(\"Processed Audio File Number: \", count)\n",
    "    \n",
    "    #We return the features and the binary target values.\n",
    "    return np.array(final_features), np.array(binary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aER6S-_k2a9H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Audio File Number:  100\n",
      "Processed Audio File Number:  200\n",
      "Processed Audio File Number:  300\n",
      "Processed Audio File Number:  400\n",
      "Processed Audio File Number:  500\n",
      "Processed Audio File Number:  600\n",
      "Processed Audio File Number:  700\n"
     ]
    }
   ],
   "source": [
    "#Please change the path below to the path of the folder saved on your computer.\n",
    "data_path = './Audio_Speech_Actors_01-24'\n",
    "X, binary_label = load_extract_features(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping binary_label to make it compatible with the train_test_split function.\n",
    "binary_label = binary_label.reshape(-1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 140)\n",
      "(768, 1)\n",
      "(537, 140)\n",
      "(231, 140)\n",
      "(537, 1)\n",
      "(231, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, binary_label, test_size=0.3, random_state=41)\n",
    "print(X.shape)\n",
    "print(binary_label.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_wrt_w(X, y, w, b, lambda_reg):\n",
    "    '''\n",
    "    gradient_wrt_w() is a function that is used to compute the gradient of the loss function with respect to the weights.\n",
    "\n",
    "    Returns:\n",
    "    1. Gradient of the loss function with respect to the weights.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    grad_w = np.zeros(w.shape)\n",
    "\n",
    "    for i in range(N):\n",
    "        if y[i]*(np.dot(w, X[i]) + b) < 1:\n",
    "            grad_w += lambda_reg*w - y[i]*X[i]\n",
    "        else:\n",
    "            grad_w += lambda_reg*w\n",
    "    return grad_w / N\n",
    "\n",
    "def gradient_wrt_b(X, y, w, b):\n",
    "    '''\n",
    "    gradient_wrt_b() is a function that is used to compute the gradient of the loss function with respect to the bias.\n",
    "\n",
    "    Returns:\n",
    "    1. Gradient of the loss function with respect to the bias.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    grad_b = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        if y[i]*(np.dot(w, X[i]) + b) < 1:\n",
    "            grad_b += -y[i]\n",
    "        else:\n",
    "            grad_b += 0\n",
    "    return grad_b / N\n",
    "\n",
    "def loss(X, y, w, b, lambda_reg):\n",
    "    '''\n",
    "    loss() is a function that is used to compute the loss function.\n",
    "\n",
    "    Returns:\n",
    "    1. Loss function value.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        if y[i]*(np.dot(w, X[i]) + b) < 1:\n",
    "            loss += 1 - y[i]*(np.dot(w, X[i]) + b)\n",
    "        else:\n",
    "            loss += 0\n",
    "    return 0.5*lambda_reg*np.dot(w.T, w) + loss / N\n",
    "\n",
    "def gradient_descent(X, y, w, b, step_size, lambda_reg, num_iterations):\n",
    "    '''\n",
    "    gradient_descent() is a function that is used to perform gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    1. Optimal Weights\n",
    "    2. Optimal Bias\n",
    "    '''\n",
    "    for i in range(num_iterations):\n",
    "        loss_pre = np.mean(loss(X, y, w, b, lambda_reg))\n",
    "        \n",
    "        # apply gradient descent step\n",
    "        grad_w = gradient_wrt_w(X, y, w, b, lambda_reg)\n",
    "        grad_b = gradient_wrt_b(X, y, w, b)\n",
    "        w -= step_size * grad_w\n",
    "        b -= step_size * grad_b\n",
    "\n",
    "        loss_post = np.mean(loss(X, y, w, b, lambda_reg))\n",
    "\n",
    "        # print every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"Loss: \", loss_post)\n",
    "        \n",
    "        # stop the algorithm if the loss is changing very little\n",
    "        if np.abs(loss_pre - loss_post) < 0.0000001:\n",
    "            break\n",
    "    return w, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Loss:  0.9055390643174237\n",
      "Iteration:  100\n",
      "Loss:  0.7209351416725835\n",
      "Iteration:  200\n",
      "Loss:  0.6880736915001773\n",
      "Iteration:  300\n",
      "Loss:  0.6834789525186824\n",
      "Iteration:  400\n",
      "Loss:  0.6805489315893753\n"
     ]
    }
   ],
   "source": [
    "# calculating optimal weights and bias using training set\n",
    "w_init, b_init = np.zeros(X_train.shape[1]), 0\n",
    "w_optimal, b_optimal = gradient_descent(X_train, y_train, w_init, b_init, step_size=0.01, lambda_reg=0.1, num_iterations=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the test data and compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6611\n",
      "Test Accuracy:  0.6753\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SET\n",
    "train_data_size = X_train.shape[0]\n",
    "y_pred_train = np.ones(train_data_size) * -1\n",
    "\n",
    "# computing prediction for training set\n",
    "for i in range(train_data_size):\n",
    "    if np.dot(w_optimal, X_train[i]) + b_optimal > 0:\n",
    "        y_pred_train[i] = 1\n",
    "\n",
    "# computing accuracy for training set\n",
    "train_correct = 0\n",
    "for i in range(train_data_size):\n",
    "    if y_pred_train[i] == y_train[i]:\n",
    "        train_correct += 1\n",
    "\n",
    "# TESTING SET\n",
    "test_data_size = X_test.shape[0]\n",
    "y_pred_test = np.ones(train_data_size) * -1\n",
    "\n",
    "# computing prediction for test set\n",
    "for i in range(test_data_size):\n",
    "    if np.dot(w_optimal, X_test[i]) + b_optimal > 0:\n",
    "        y_pred_test[i] = 1\n",
    "\n",
    "# computing accuracy for test set\n",
    "correct = 0\n",
    "for i in range(test_data_size):\n",
    "    if y_pred_test[i] == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "# print accuracy in 2 decimal places\n",
    "print(\"Training Accuracy: \", round(train_correct / train_data_size, 4))\n",
    "print(\"Test Accuracy: \", round(correct / test_data_size, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Data using PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components:  13\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "\n",
    "# Compute the covariance matrix of the dataset\n",
    "cov_mat = np.cov(X.T)\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors of the covariance matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "# Sort the eigenvalues and eigen vectors in descending order\n",
    "eig_vals_sorted = np.sort(eig_vals)[::-1]\n",
    "eig_vecs_sorted = eig_vecs[:, eig_vals.argsort()[::-1]]\n",
    "\n",
    "# determine the number of principal components\n",
    "cut_off = 0.95 # 95% of the variance\n",
    "k = -1\n",
    "\n",
    "sum_of_eig_vals = sum(eig_vals)\n",
    "eig_val_percentage_var = []\n",
    "for i in eig_vals:\n",
    "    eig_val_percentage_var.append(i/sum_of_eig_vals)\n",
    "\n",
    "cumulative_sum = 0\n",
    "for i in range(len(eig_val_percentage_var)):\n",
    "    cumulative_sum += eig_val_percentage_var[i]\n",
    "    if cumulative_sum >= cut_off:\n",
    "        k = i + 1\n",
    "        break\n",
    "\n",
    "assert k != -1, \"k is not set\"\n",
    "print(\"Number of principal components: \", k)\n",
    "\n",
    "# Compute the matrix with k eigenvectors associated with the k largest eigenvalues\n",
    "W = eig_vecs_sorted[:, :k]\n",
    "\n",
    "# Compute the new dataset\n",
    "new_X = np.dot(X - mean, W)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resplit the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 13)\n",
      "(768, 1)\n",
      "(537, 13)\n",
      "(231, 13)\n",
      "(537, 1)\n",
      "(231, 1)\n"
     ]
    }
   ],
   "source": [
    "new_X_train, new_X_test, y_train, y_test = train_test_split(new_X, binary_label, test_size=0.3, random_state=41)\n",
    "print(new_X.shape)\n",
    "print(binary_label.shape)\n",
    "print(new_X_train.shape)\n",
    "print(new_X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Loss:  0.9052105553409516\n",
      "Iteration:  100\n",
      "Loss:  0.6842276064975295\n",
      "Iteration:  200\n",
      "Loss:  0.680602334061041\n",
      "Iteration:  300\n",
      "Loss:  0.6792663108266637\n",
      "Iteration:  400\n",
      "Loss:  0.678128373294901\n",
      "Iteration:  500\n",
      "Loss:  0.6774215498611997\n",
      "Iteration:  600\n",
      "Loss:  0.6768703250950927\n",
      "Iteration:  700\n",
      "Loss:  0.6762519756616299\n",
      "Iteration:  800\n",
      "Loss:  0.6760084502791516\n",
      "Iteration:  900\n",
      "Loss:  0.6755019919893898\n"
     ]
    }
   ],
   "source": [
    "# calculating optimal weights and bias using reduced training set\n",
    "w_init, b_init = np.zeros(new_X_train.shape[1]), 0\n",
    "w_optimal, b_optimal = gradient_descent(new_X_train, y_train, w_init, b_init, step_size=0.01, lambda_reg=0.1, num_iterations=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the SVM to the reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6574\n",
      "Test Accuracy:  0.671\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SET\n",
    "train_data_size = new_X_train.shape[0]\n",
    "y_pred_train = np.ones(train_data_size) * -1\n",
    "\n",
    "# computing prediction for training set\n",
    "for i in range(train_data_size):\n",
    "    if np.dot(w_optimal, new_X_train[i]) + b_optimal > 0:\n",
    "        y_pred_train[i] = 1\n",
    "\n",
    "# computing accuracy for training set\n",
    "train_correct = 0\n",
    "for i in range(train_data_size):\n",
    "    if y_pred_train[i] == y_train[i]:\n",
    "        train_correct += 1\n",
    "\n",
    "# TESTING SET\n",
    "test_data_size = new_X_test.shape[0]\n",
    "y_pred_test = np.ones(train_data_size) * -1\n",
    "\n",
    "# computing prediction for test set\n",
    "for i in range(test_data_size):\n",
    "    if np.dot(w_optimal, new_X_test[i]) + b_optimal > 0:\n",
    "        y_pred_test[i] = 1\n",
    "\n",
    "# computing accuracy for test set\n",
    "correct = 0\n",
    "for i in range(test_data_size):\n",
    "    if y_pred_test[i] == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "# print accuracy in 2 decimal places\n",
    "print(\"Training Accuracy: \", round(train_correct / train_data_size, 4))\n",
    "print(\"Test Accuracy: \", round(correct / test_data_size, 4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
